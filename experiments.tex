\newpage
\section{Experiments}

% \section{Baseline}

We compare our model to the vanilla logistic regression algorithm by implementing both using the TensorFlow library. Both the models can perform multi-class and multi-label classification. For multi-class, we use the softmax function to assign probabilities to each class which add up to 1. We use the sigmoid function for multi-label classification to train an independent logistic regression model for each class and accept all labels greater than a certain threshold as predictions.

We tune our model using learning rate and regularization parameter as hyper-parameters and train with different number of epochs till the loss converges. For multi-label problems, we also use prediction threshold as one of the hyper-parameters to list labels with probabilities greater than the threshold value.

\subsection{Evaluation}

To evaluate the performance of both models for multi-class problems, we use accuracy and F1 measures.

For multi-label problems, predictions for an instance is a set of labels, and therefore the prediction can be fully correct, partially-correct or fully-incorrect. Thus, to better evaluate our models we will use the following three measures: \textit{set accuracy}, which is the ratio of perfectly matched instances to the total number of instances; \textit{instance-F1}, which evaluates the performance of partially correct predictions averaged over instances; \textit{label-F1}, which evaluates the performance of partially correct predictions averaged over labels.\\

For a dataset with ground truth labels $y^{(n)}$ and predictions $\hat{y}^{(n)}$, and n instances where n = 1,2,...,N, these three measures are defined as:

\begin{alignat}{2}
\label{eq:eval}
set\; accuracy & = \frac{1}{N} \sum_{n=1}^{N} {I(y^{(n)} = \hat{y}^{(n)})}\\
instance \mbox{-} F1 & = \frac{1}{N} \sum_{n=1}^{N} \frac{2\sum_{l=1}^{L} {y^{(n)}_{l} \hat{y}^{(n)}_{l}}}{\sum_{l=1}^{L} {y^{(n)}_{l}} + \sum_{l=1}^{L} {\hat{y}^{(n)}_{l}}}
\hspace{6ex}
label \mbox{-} F1 & = \frac{1}{N} \sum_{n=1}^{N} \frac{2\sum_{n=1}^{N} {y^{(n)}_{l} \hat{y}^{(n)}_{l}}}{\sum_{n=1}^{N} {y^{(n)}_{l}} + \sum_{n=1}^{N} {\hat{y}^{(n)}_{l}}}
\end{alignat}

\noindent where for each instance $n$, $y^{(n)}_{l}$ = 1 if label $l$ is a given label in ground truth; \\
$\hat{y}^{(n)}_{l}$ = 1 if label $l$ is a predicted label.
\newpage
\subsection{IMDb}

The IMDb dataset is created by Meka with movie plot text summaries labelled with genres sourced from the Internet Movie Database Interface. It is a collection of 35,000 documents partitioned across 25 different genres. The task is to assign multiple genres to a movie description.

Below is a list of the 25 different genres with individual genre counts.

\begin{table}[htbp]
\centering
\begin{tabular}{ll|ll|ll}
Drama & 11726 & Mystery & 1775 & Musical & 687 \\
Comedy & 7279 & Sci-Fi & 1692 & Western & 574 \\
Romance & 4488 & Fantasy & 1612 & Short & 546 \\
Thriller & 4468 & Family & 1564 & Sport & 471 \\
Crime & 3195 & Biography & 1151 & Film-Noir & 257 \\
Action & 3120 & War & 1049 & Reality-TV & 68 \\
Horror & 2414 & Animation & 949 & News & 12 \\
Adventure & 2313 & History & 932 &  &  \\
Documentary & 1895 & Music & 827 &  & 
\end{tabular}
\caption{\label{tab:widgets}The 24 topic categories for the IMDb dataset with the number of examples assigned to them.}
\end{table}

\subsubsection{Model Performance on IMDb}

We evaluate how the model performed on the IMDb dataset by comparing its performance with the vanilla logistic regression model. We compare both the overall model performance as well as the individual label performance.\\

\noindent \textbf{Individual Label Performance}\\

Below is a table showing the individual label performance between both models which has been sorted in descending order of its performance improvement.\\

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
News                       & 0.00                                                                & 50.00                                                           & 50.00                                                             \\
Musical                    & 33.54                                                               & 43.78                                                           & 10.24                                                             \\
Animation                  & 44.66                                                               & 50.23                                                           & 5.57                                                              \\
Sport                      & 61.65                                                               & 67.13                                                           & 5.47                                                              \\
Film Noir                  & 25.31                                                               & 30.23                                                           & 4.91                                                              \\
War                        & 58.64                                                               & 62.92                                                           & 4.27                                                              \\
Sci-Fi                     & 68.83                                                               & 72.73                                                           & 3.89                                                             
\end{tabular}
\caption{\label{tab:widgets}IMDb Model Performance at a Label level.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
Adventure                  & 52.44                                                               & 56.14                                                           & 3.7                                                               \\
Music                      & 54.08                                                               & 56.98                                                           & 2.9                                                               \\
Crime                      & 60.24                                                               & 62.72                                                           & 2.48                                                              \\
Documentary                & 71.05                                                               & 73.36                                                           & 2.31                                                              \\
Short                      & 56.06                                                               & 58.23                                                           & 2.17                                                              \\
Family                     & 51.07                                                               & 53.21                                                           & 2.14                                                              \\
Comedy                     & 62.87                                                               & 64.67                                                           & 1.8                                                               \\
Romance                    & 51.17                                                               & 52.5                                                            & 1.33                                                              \\
Fantasy                    & 41.96                                                               & 43.05                                                           & 1.09                                                              \\
Mystery                    & 47.91                                                               & 48.61                                                           & 0.7                                                               \\
Horror                     & 64.23                                                               & 64.8                                                            & 0.57                                                              \\
History                    & 38.01                                                               & 38.33                                                           & 0.32                                                              \\
Western                    & 77.21                                                               & 77.38                                                           & 0.17                                                              \\
Reality-TV                 & 0                                                                   & 0                                                               & 0                                                                 \\
Biography                  & 39.36                                                               & 38.17                                                           & -1.19                                                             \\
Drama                      & 73.45                                                               & 72.24                                                           & -1.21                                                             \\
Thriller                   & 54.86                                                               & 51.81                                                           & -3.05                                                             \\
Action                     & 18.18                                                               & 11.54                                                           & -6.64                                                            
\end{tabular}
\caption{\label{tab:widgets}IMDb Model Performance at a Label level.}
\end{table}

\newpage
\noindent \textbf{Deeper analysis of label - News:}\\

From the above table we can see that the vanilla logistic regression model performed very badly on the \textbf{News} dataset as compared to our new model. Upon further investigating the dataset, we found that this particular label had only 12 records in total, out of which 9 were train and 3 were test.

Below was the train and test performance of both the models on the \textbf{News} label.

\begin{table}[htbp]
\centering
\begin{tabular}{l|l|l|l|l|l|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 0.00 & 22412 & 0 & 9 & 0 \\
\textbf{test} & 0.00 & 3114 & 1 & 3 & 0
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{News} Label.}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{l|l|l|l|l|l|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 100.00 & 22412 & 0 & 0 & 9 \\
\textbf{test} & 50.00 & 3115 & 0 & 2 & 1
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{News} Label.}
\end{table}

Upon just looking at the train performance of both models, we can see that the vanilla logistic regression model has predicted 0 true positives, while our model predicted all 9 true positives correctly and has an F1 score of 100\%. Predictably looking at the train performance, the vanilla model did not perform well on the test set as compared to the word-vector model. This example proves that the word-vector model can perform well even with very few training records.\\

\noindent \textbf{Deeper analysis of label - Musical:}\\

Let's look at another label \textbf{Musical} which has more positive examples than the previous case. This particular label had 687 train and 109 test records and below is the model performance on both the train and test splits for both the vanilla logistic regression and the word-vector models.

\begin{table}[htbp]
\centering
\begin{tabular}{l|l|l|l|l|l|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 82.88 & 21730 & 4 & 198 & 489 \\
\textbf{test} & 33.54 & 2984 & 25 & 82 & 27
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{Musical} Label.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|l|l|l|l|l|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 85.07 & 21724 & 10 & 171 & 516 \\
\textbf{test} & 43.79 & 2986 & 23 & 72 & 37
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{Musical} Label.}
\end{table}

From the above tables, we can see that the word-vector model was able to predict correctly more true positives and lesser false negatives on the train dataset. This performance carried over to the test set as well. Also looking at the train and test F1 scores of both models, we can see that there is a bigger difference in train-test scores of the logistic regression model as compared to the word-vector one. We noticed this trend in general where the word-vector model is less likely to overfit.\\

\noindent \textbf{Deeper analysis of label - Action:}\\

Let's look at a label from the bottom end of the table - \textbf{Action}. This label had 257 train and 44 test documents and below is how the model performed.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 86.53 & 22164 & 0 & 61 & 196 \\
\textbf{test} & 18.18 & 3068 & 6 & 39 & 5
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{Action} Label.}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 84.49 & 22164 & 0 & 69 & 188 \\
\textbf{test} & 11.54 & 3069 & 5 & 41 & 3
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{Action} Label.}
\end{table}

Looking at the performance we can see that the word-vector model predicted more false negatives as compared to the vanilla model. However looking at the test performance, we can see that  our model predicted only 2 fewer true positives as compared to the vanilla model. This might've been caused due to fewer training samples or it was a difficult label to train.

\newpage
\noindent \textbf{Overall Model Performance}\\

So far we've looked at how the model performed at an individual label level. Now let's look at how the overall model performed. Since each document in the IMDb dataset can belong to multiple genres, we're trying to solve a multi-label classification problem. For multi-label classification, as discussed previously in equation \ref{eq:eval}, we will use Set-Accuracy and Instance-F1 measures to evaluate the model. Along with evaluating the model on the vanilla logistic regression and our word-vector model, we also evaluate the model on pyramid's version of logistic regression. Below is how each of the model's performed.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c}
Datasets & \begin{tabular}[c]{@{}c@{}}Pyramid LR\\ Set Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Set Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our model\\ Set Accuracy\end{tabular}\\\hline
IMDb & 19.66 & 18.21 & 20.30\\
\end{tabular}
\caption{\label{tab:widgets}Set-Accuracy Results.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c}
Datasets & \begin{tabular}[c]{@{}c@{}}Pyramid LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our model\\ Instance-F1\end{tabular}\\\hline
IMDb & 56.14 & 57.87 & 58.52\\
\end{tabular}
\caption{\label{tab:widgets}Instance-F1 Results.}
\end{table}

Looking at the results, we can see that our model has outperformed both the pyramid and vanilla logistic regression models on both Set-Accuracy and Instance-F1.

\newpage
\subsection{Guardian}

The Guardian is a National British daily newspaper, known until 1959 as the Manchester Guardian. Its online edition was the fifth most widely read in the world as of October 2014, with over 42 million readers. This dataset contains articles on various topics like World news, UK news, Culture, Politics, Media, Business, Society etc.

All the data had been manually scrapped from the Guardian website with each file containing a document and its associated tags. The dataset in use contains 20 different classes and the task is to assign multiple tags to each news article.

Below is a list of the 20 different genres.

\begin{table}[htbp]
\centering
\begin{tabular}{l|l}
Blogging       & LGBT rights    \\
Christianity      & Mental health \\
Comedy     & Poetry       \\
Computing    & Premier League \\
Counter-terrorism policy      & Public finance   \\
Drugs      & Public sector cuts     \\
Financial crisis      & Research   \\
Global enonomy   & Restaurants   \\
Health policy & Retail industry     \\
Inequality     & Tax and spending      
\end{tabular}
\caption{\label{tab:widgets}The 20 topic categories for the Guardian dataset with the number of examples assigned to them.}
\end{table}

\noindent \textbf{Individual Label Performance}\\

Below is a table showing the individual label performance between both models which has been sorted in descending order of its performance improvement. For this dataset, we did not get any labels that performed badly on the word-vector model as compared to the logistic regression model.\\

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
Drugs                      & 80.5                                                             & 88.75                                                        & 8.25                                                              \\
Mental health              & 69.23                                                            & 76.8                                                         & 7.57                                                              \\
Global economy             & 56.34                                                            & 62.69                                                        & 6.35                                                              \\
LGBT                       & 76.84                                                            & 82.9                                                         & 6.06                                                              \\
Blogging                   & 77.3                                                             & 82.72                                                        & 5.42                                                              \\
Public finance             & 48.24                                                            & 53.33                                                        & 5.09                                                              \\
Premier League             & 90                                                               & 95.03                                                        & 5.03                                                              \\
Research                   & 60.97                                                            & 65.84                                                        & 4.87                                                              \\
Retail                     & 79.78                                                            & 84.62                                                        & 4.84                                                             
\end{tabular}
\caption{\label{tab:widgets}Guardian Model Performance at a Label level.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
Inequality                 & 50.3                                                             & 55.12                                                        & 4.82                                                              \\
Tax and Spending           & 50.73                                                            & 54.35                                                        & 3.62                                                              \\
Counter-terrorism          & 88.88                                                            & 91.5                                                         & 2.62                                                              \\
Restraunts                 & 88.72                                                            & 91.05                                                        & 2.33                                                              \\
Health policy              & 68.08                                                            & 70.27                                                        & 2.19                                                              \\
Poetry                     & 89.17                                                            & 91.14                                                        & 1.97                                                              \\
Financial Crisis           & 49.43                                                            & 50.87                                                        & 1.44                                                              \\
Computing                  & 69.82                                                            & 71.01                                                        & 1.19                                                              \\
Public sector cuts         & 56.83                                                            & 57.79                                                        & 0.96                                                              \\
Comedy                     & 77.1                                                             & 77.65                                                        & 0.55                                                              \\
Christianity               & 75                                                               & 75.47                                                        & 0.47                                                             
\end{tabular}
\caption{\label{tab:widgets}Guardian Model Performance at a Label level.}
\end{table}

\noindent \textbf{Deeper analysis of label - Drugs:}\\

Let's look at the model with the biggest F1 gains \textbf{Drugs}. This label had 625 positive samples of news articles labelled \textbf{Drugs}. Out of these 545 belonged to the trainset and 80 belonged to the testset. Below was the train and test performance of both the models on the \textbf{Drugs} label.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 92.46 & 10632 & 19 & 60 & 485 \\
\textbf{test} & 80.50 & 1463 & 15 & 16 & 64
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{Drugs} Label.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 94.91 & 10639 & 12 & 42 & 503 \\
\textbf{test} & 88.75 & 1469 & 9 & 9 & 71
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{Drugs} Label.}
\end{table}

From the above metrics, we can see that the word-vector model got more true positives and fewer false negatives, false positives as compared to the vanilla logistic regression model leading to higher F1 score. This performance improvement can be observed on both the train and test datasets.\\

\noindent \textbf{Deeper analysis of label - Global Economy:}\\

Looking at another label \textbf{Global economy}, which has 605 train and 72 test samples and juxtaposing their model performances. Comparing just the train performance of both models, we can see that the vanilla model got almost 20 true positives right as compared to the word-vector model. However, when you look at their corresponding test performances, we can that the word-vector model got 2 extra true positives right as compared to the vanilla version. This shows that the vanilla model was clearly overfitting on the trainset and performed badly on the testset, while the word-vector model is not so much prone to overfitting. Hence we see a much higher test F1 score for the word-vector model as compared to the vanilla model.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 87.36 & 10541 & 50 & 97 & 508 \\
\textbf{test} & 56.33 & 1456 & 30 & 32 & 40
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{Global economy} Label.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \multicolumn{1}{l|}{\textbf{F1}} & \multicolumn{1}{l|}{\textbf{TN}} & \multicolumn{1}{l|}{\textbf{FP}} & \multicolumn{1}{l|}{\textbf{FN}} & \multicolumn{1}{l|}{\textbf{TP}} \\ \hline
\textbf{train} & 88.31 & 10575 & 16 & 114 & 491 \\
\textbf{test} & 62.69 & 1466 & 20 & 30 & 42
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{Global economy} Label.}
\end{table}

\noindent \textbf{Deeper analysis of label - Christianity:}\\

Analyzing a label from the bottom of the table we see that the logistic regression model already had a very high train F1 score and the word-vector model was not able to improve much on this.

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 91.75 & 10461 & 37 & 75 & 623 \\
\textbf{test} & 75.00 & 1453 & 13 & 29 & 63
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on \textbf{Christianity} Label.}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 93.39 & 10478 & 20 & 69 & 629 \\
\textbf{test} & 75.47 & 1459 & 7 & 32 & 60
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on \textbf{Christianity} Label.}
\end{table}

\newpage
\noindent \textbf{Overall Model Performance}\\

Like the IMDb dataset, each document of the Guardian dataset can belong to multiple tags. So this will also be a multi-label classification problem, and we will be using Set-Accuracy and Instance-F1 measures to evaluate the model. Along with evaluating the model on the vanilla logistic regression and our word-vector model, we also evaluate the model on pyramid's version of logistic regression. Below is how each of the model's performed.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c}
\multicolumn{1}{c|}{Datasets} & \begin{tabular}[c]{@{}c@{}}Pyramid LR\\ Set Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Set Accuracy\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our model\\ Set Accuracy\end{tabular} \\ \hline
Guardian                      & 59.3                                                              & 53.46                                                                & 59.3                                                            
\end{tabular}
\caption{\label{tab:widgets}Set-Accuracy Results.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c}
\multicolumn{1}{c|}{Datasets} & \begin{tabular}[c]{@{}c@{}}Pyramid LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our model\\ Instance-F1\end{tabular} \\ \hline
Guardian                      & 65.13                                                            & 65.61                                                               & 69.41                                                          
\end{tabular}
\caption{\label{tab:widgets}Instance-F1 Results.}
\end{table}

Looking at the results, we can see that our model has outperformed both the pyramid and vanilla logistic regression models on both Set-Accuracy and Instance-F1.

\newpage
\subsubsection{20 Newsgroups}

The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.

The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g.\textbf{ comp.sys.ibm.pc.hardware / comp.sys.mac.hardware}), while others are highly unrelated (e.g \textbf{misc.forsale / soc.religion.christian}). Except for a small fraction of the articles, each document belongs to exactly one newsgroup. The task is to learn which newsgroup an article was posted to. Below is a list of 20 newsgroup partitioned according to their subject matter.\\

\begin{table}[htbp]
\begin{tabular}{|l|l|l|lllllll}
\cline{1-3}
comp.graphics            &                       &                        &  &  &  &  &  &  &  \\
comp.os.ms-windows.misc  & rec.autos             & sci.crypt              &  &  &  &  &  &  &  \\
comp.sys.ibm.pc.hardware & rec.motorcycles       & sci.electronics        &  &  &  &  &  &  &  \\
comp.sys.mac.hardware    & rec.sport.baseball    & sci.med                &  &  &  &  &  &  &  \\
comp.windows.x           & rec.sport.hockey      & sci.space              &  &  &  &  &  &  &  \\ \cline{1-3}
                         & talk.politics.misc    & talk.religion.misc     &  &  &  &  &  &  &  \\
misc.forsale             & talk.politics.guns    & alt.atheism            &  &  &  &  &  &  &  \\
                         & talk.politics.mideast & soc.religion.christian &  &  &  &  &  &  &  \\ \cline{1-3}
\end{tabular}
\caption{\label{tab:widgets}Newsgroups used in newsgroups data}
\end{table}

\subsubsection{Model Performance on 20 Newsgroup}

We evaluate how the model performed on the 20 Newsgroup dataset by comparing its performance with the vanilla logistic regression model. We compare both the overall model performance as well as the individual label performance.\\

\noindent \textbf{Individual Label Performance}\\

Below is a table showing the individual label performance between both models which has been sorted in descending order of its performance improvement.\\

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
comp.sys.ibm.pc.hardware   & 48.98                                                            & 56.68                                                        & 7.7                                                               \\
soc.religion.christian     & 58.24                                                            & 64.09                                                        & 5.85                                                              \\
talk.politics.misc         & 46.45                                                            & 51.9                                                         & 5.45                                                              \\
comp.os.ms-windows.misc    & 55.17                                                            & 60.3                                                         & 5.13                                                              \\
comp.graphics              & 51.81                                                            & 56                                                           & 4.19                                                              
\end{tabular}
\caption{\label{tab:widgets}20 Newsgroup Model Performance at a Label level.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
\multicolumn{1}{c|}{Label} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our Model\\ Label F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Percentage \\ Improvement\end{tabular} \\ \hline
comp.windows.x             & 67                                                               & 71                                                           & 4                                                                 \\
sci.electronics            & 51.04                                                            & 55.03                                                        & 3.99                                                              \\
comp.sys.mac.hardware      & 54.8                                                             & 57.83                                                        & 3.03                                                              \\
alt.atheism                & 44.44                                                            & 46.27                                                        & 1.83                                                              \\
sci.space                  & 66.67                                                            & 67.82                                                        & 1.15                                                              \\
rec.sport.baseball         & 72.63                                                            & 73.3                                                         & 0.67                                                              \\
talk.politics.guns         & 55.17                                                            & 55.81                                                        & 0.64                                                              \\
talk.religion.misc         & 37.21                                                            & 37.68                                                        & 0.47                                                              \\
sci.med                    & 68.13                                                            & 68.06                                                        & -0.07                                                             \\
sci.crypt                  & 75.36                                                            & 75.12                                                        & -0.24                                                             \\
rec.autos                  & 53.95                                                            & 53.5                                                         & -0.45                                                             \\
talk.politics.mideast      & 76.65                                                            & 76.02                                                        & -0.63                                                             \\
misc.forsale               & 67.84                                                            & 66.09                                                        & -1.75                                                             \\
rec.motorcycles            & 60.47                                                            & 58.7                                                         & -1.77                                                             \\
rec.sport.hockey           & 86.17                                                            & 84.21                                                        & -1.96                                                            
\end{tabular}
\caption{\label{tab:widgets}20 Newsgroup Model Performance at a Label level.}
\end{table}

\noindent \textbf{Deeper analysis of label - comp.sys.ibm.pc.hardware:}\\

Let's look at the model with the biggest F1 gains \textbf{comp.sys.ibm.pc.hardware}. This label had 797 positive samples. Out of these 696 belonged to the trainset and 101 belonged to the testset. Below was the train and test performance of both the models on the \textbf{comp.sys.ibm.pc.hardware} label.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 95.81 & 12435 & 1 & 55 & 641 \\
\textbf{test} & 48.98 & 1782 & 47 & 53 & 48
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on the \textbf{comp.sys.ibm.pc.hardware} label.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 98.54 & 12436 & 0 & 20 & 676 \\
\textbf{test} & 56.68 & 1796 & 33 & 48 & 53
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on the \textbf{comp.sys.ibm.pc.hardware} label.}
\end{table}

We can see that the word-vector model got much higher true positives and fewer false negatives as compared to the vanilla model for the trainset. And this performance is also reflected in the testset.


\newpage
\noindent \textbf{Deeper analysis of label - talk.politics.misc:}\\

Analyzing the performance of another label \textbf{talk.politics.misc} which has 636 total samples out of which 548 are train and 88 are test. Below is how both the models performed on the \textbf{talk.politics.misc} label.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 96.03 & 12582 & 2 & 40 & 508 \\
\textbf{test} & 46.45 & 1811 & 31 & 52 & 36
\end{tabular}
\caption{\label{tab:widgets}Vanilla logistic regression train-test performance on the \textbf{talk.politics.misc} label.}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|c|c|}
 & \textbf{F1} & \textbf{TN} & \textbf{FP} & \textbf{FN} & \textbf{TP} \\ \hline
\textbf{train} & 97.95 & 12584 & 0 & 22 & 526 \\
\textbf{test} & 51.9 & 1813 & 29 & 47 & 41
\end{tabular}
\caption{\label{tab:widgets}Word-vector model train-test performance on the \textbf{talk.politics.misc} label.}
\end{table}

From the above performance results, we can see that the vanilla logistic regression model already had a very high train F1 score. In-spite of that, the word-vector model managed to further improve it by predicting more true positives correctly.\\

\noindent \textbf{Overall Model Performance}\\

Each document in the belongs to one of the 20 classes. So unlike the IMDb and Guardian dataset where we were solving multi-label classification, for this dataset we are solving a multi-class classification problem. So we do not have metrics like Set-Accuracy or Instance-F1 to compute the model performance. We instead will only use the F1 score over the entire dataset. Below is how each of the models performed.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
Datasets & \begin{tabular}[c]{@{}c@{}}Pyramid-LR\\ F1 Score\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow-LR\\ F1 Score\end{tabular} & \begin{tabular}[c]{@{}c@{}}Our model\\ F1 Score\end{tabular} \\ \hline
20 Newsgroup & 55.64 & 55.75 & 58.80
\end{tabular}
\caption{\label{tab:widgets}Overall F1 Results.}
\end{table}

Looking at the results, we can see that our model has outperformed the pyramid and vanilla logistic regression models.


\newpage
\subsection{Additional tests}

We also ran a validation experiment with artificially modified dataset on the 20 newsgroup dataset to mimic sparse representation: To do this we found the most similar words in the train set with cosine similarity greater than 0.3 and make the feature values of all but one of the words equal to zero. We then compared the performance of our model with logistic regression before and after zeroing feature values of similar words. The below results show that our model was able to get a good test accuracy after the imposed sparsity.\\


\begin{table}[htbp]
\centering
\begin{tabular}{l|l|l}
 & \multicolumn{1}{c|}{Logistic regression} & \multicolumn{1}{c}{Our model} \\ \hline
Before zeroing features & 82.6 & 84.01 \\
After zeroing features & 75.67 & 83.54
\end{tabular}
\caption{\label{tab:widgets}Model performance on sparse data}
\end{table}
