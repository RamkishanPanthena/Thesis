\begin{abstract}
A simple and efficient baseline for text classification is to represent sentences as bag-of-words (BoW) and train a linear classifier. The bag-of-words model is simple to implement and offers flexibility for customization by providing different scoring techniques for user specific text data. In many problem domains, these linear classifiers are preferred over more complex models like CNN, LSTM because of their efficiency, robustness and interpretability.

However, a large vocabulary can cause extremely sparse representations which are harder to model, where the challenge is for models to harness very little information in such a large representational space. Also, these classification problems are categorized by large number of classes and highly imbalanced distribution of data across these classes. In such cases, the traditional linear classifiers would treat each word separately and assign them different coefficients based on the frequency in which they occur in the train set. This would result in lower test accuracy when it comes across instances where a word which was occurring less frequently in the train set, occurs more often in the test set.

Our thesis aims to solve this problem by constraining weights of rare features by similar, more frequent ones, using semantic similarity. This would enforce similar words to have similar weights thereby improving model performance. Thus, based on how similar two features are, our proposed model can improve the feature importance of a sparse word by increasing its regression co-efficient, thereby improving the test accuracy in the above mentioned scenario.

\end{abstract}