\newpage
\section{Training neural network models with single hidden layer}

\begin{figure}[htbp]
\centering
\includegraphics[width=16cm, height=10cm]{images/nn1.png}\\
\centering
\caption{Neural network model - without using word-vectors}
\label{fig:foo}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=16cm, height=10cm]{images/nn2.png}\\
\centering
\caption{Neural network model - with word-vectors}
\label{fig:foo}
\end{figure}

\newpage
\subsection{Model Performance}

Below is a table showing how a neural network model with one hidden layer performed with and without using features generated by word-vectors.

\begin{table}[htbp]
\centering
\begin{tabular}{llllll}
\multicolumn{1}{l|}{Datasets}    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}1 layer NN\\ Set Accuracy\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}1 layer NN with wv\\ Set Accuracy\end{tabular}} &  &  &  \\ \cline{1-3}
\multicolumn{1}{l|}{IMDb}        & \multicolumn{1}{c|}{13.2}                                                              & \multicolumn{1}{c|}{19.63}                                                                     &  &  &  \\
\multicolumn{1}{l|}{20NewsGroup} & \multicolumn{1}{c|}{49.23}                                                                  & \multicolumn{1}{c|}{59.89}                                                                     &  &  &  \\
\end{tabular}
\caption{\label{tab:widgets}Set-Accuracy Results}
\end{table}


\begin{table}[htbp]
\centering
\begin{tabular}{llllll}
\multicolumn{1}{l|}{Datasets}    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}1 layer NN\\ Instance F1\end{tabular}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}1 layer NN with wv\\ Instance F1\end{tabular}} &  &  &  \\ \cline{1-3}
\multicolumn{1}{l|}{IMDb}        & \multicolumn{1}{c|}{51.13}                                                            & \multicolumn{1}{c|}{60.21}                                                                    &  &  &  \\
\multicolumn{1}{l|}{20NewsGroup} & \multicolumn{1}{c|}{48.71}                                                                 & \multicolumn{1}{c|}{58.97}                                                                    &  &  &  \\
\end{tabular}
\caption{\label{tab:widgets}Instance F1 Results}
\end{table}

We can see that for both the datasets, the neural network model using features generated by word-vectors is the better performing model. In fact this model even outperforms all other previously tested models.

\newpage
\section{Other models tested - Vanilla logistic regression with word-vector regularization}

Our proposed method trained a model using coefficients trained from both the vanilla logistic regression model and word-vectors. In this model, instead of generating the feature weights using the word-vectors, we use the word-vectors as a regularizer that is added to the cost function. This regularizer would add a higher penalty to similar words having large differences in their corresponding feature weights. This kind of regularization should technically enforce similar words to have similar weights.

Thus, similar to logistic regression the probability the class variable $y_{j}=1, j=1,2,...m$ can be modelled as follows:

\begin{equation}
\ P(y_{j}  = 1 | x_{j}; \theta) = h_{\theta}(x_{j}) = \frac{1}{1+e^{-\theta^{T}x_{(j)}}}
\end{equation}

Using the principle of maximum likelihood estimate, we find the parameters that maximize the likelihood P(X $|$ y). Hence the log-likelihood is given as,

\begin{equation}
\ L(\theta) = \sum_{i=1}^{m}{y_{i}log(h_{\theta}(x_{i})) + (1-y_{i})log(1-h_{\theta}(x_{i}))}
\end{equation}

Maximizing the log-likelihood is similar to minimizing -L($\theta$) over all data points. The cost function for the logistic regression model comes out to be,

\begin{equation}
\ J(\theta) = -\frac{1}{m}{L(\theta)}
\end{equation}

Adding L1, L2 and word-vector regularization to this we get:

\begin{equation}
\ J(\theta) = -\frac{1}{m}{L(\theta)} + \frac{\lambda_{1}}{m}{|{\theta}|} + \frac{\lambda_{2}}{m}{|{\theta}|}^{2} + 
\frac{\lambda_{3}}{m}{(WV Reg)}
\end{equation}

where $\lambda_{1}, \lambda_{2}, \lambda_{3}$ and l1, l2 and word-vector regularization constraints.\\

\noindent Given a dataset of $n$ features, we compute the word-vector regularization as follows:

\begin{itemize}
\item Compute the cosine similarity between all feature's words-vectors. This will give us a $(n$ x $n)$ dimensional similarity matrix
\item Compute the difference between the corresponding feature weights $\theta$. This will also give us an $(n$ x $n)$ dimensional matrix that contains the difference between a feature's weight to all other weights
\item Perform a matrix multiplication between the similarity matrix and the matrix having the difference between the coefficients
\item The final result will act as a word-vector regularizer which can be added to the overall cost function regularized by a regularization parameter $\lambda_{3}$

\end{itemize}

\newpage

\begin{figure}[htbp]
\centering
\includegraphics[width=16cm, height=10cm]{images/model3.png}\\
\centering
\caption{Logistic Regression with Word-vector regularization}
\label{fig:foo}
\end{figure}

\subsection{Why would it work?}

Let's say there are two words "Russia" and "Soviet" having similar word-vectors due to their reference in similar context. However the word "Russia" is present in the text more often than "Soviet" and has higher feature weight. Due to their high similarity and larger difference in weights, the word-vector model will add a higher penalty to the cost function until their corresponding weights are close to each other.

Now the claim is that the cost function would be minimized if the word-vector penalty is minimum. Now let's think about a few cases: 

\begin{itemize}
    \item \textbf{Case 1:} If two features $x_{i}$, $x_{j}$ are completely dissimilar, their cosine similarity is 0, and they don't contribute to the cost function
    \item \textbf{Case 2:} If two features are completely alike, their cosine similarity is 1. Now we have two subcases:
    \begin{itemize}
        \item \textbf{Subcase 1:} Both features have similar weights, thus the difference between their weights is 0. This would also not contribute to the cost function.
        
        \item \textbf{Subcase 2:} Both features have very different weights, thus the difference between their weights would be large. This would \textit{increase} the overall cost function. This is precisely the examples we are looking for: similar features having very different weights. So the algorithm forces them to have the same weights.
    \end{itemize}
\end{itemize}

\subsection{Challenges faced and workarounds}

The major challenge faced while running this model is the amount of time it took to generate the weight differences matrix and computing its dot product with the similarity matrix. Since the model was trained using mini-batch gradient descent, we had to compute the word-vector regularization cost at the end of every mini-batch in order to update the weights. 

So for a medium sized dataset with 10,000 features, 100,000 records and a decent batch size of 10\% of training data, one would have to compute these matrices and get their dot product at least 10 times. And that would train the model for just a single epoch. For training a good enough model with a decent learning rate, one would need to train the model for at least 400-500 epochs. All of this only if we are training a binary classifier. If we are training a multi-class or multi-label classifier with $\sim$2000 different classes, then it would take an exponential amount of time. This puts a huge constraint on the amount of time and resources needed to train the model. For training an even larger dataset with $\sim$100,000 features, it would be even more difficult as training a single epoch itself might take a few hours.

\subsubsection{Workarounds}

In order to remediate this, we limit the features for which we want to increase the weights. Since our objective through this project was to make rare features have higher weights, we can pick the top k rarest features and look at their similarities with the rest of the features. This would reduce the similarity matrix size from $(n$ x $n)$ to $(n$ x $k)$. We can choose the top rare features based on their IDF (Inverse Document Frequency) score.

Inverse docment frequency or IDF is a numerical statistic used in information retrieval to reflect how rare a term is. It is known that certain terms, such as "is", "of", and "that" may appear a lot of times but have little importance. IDF helps in weighing down the frequent terms and finding the rare ones by computing the following:

\begin{equation}
\ IDF = log_e (\frac{Total number of documents}{Number of documents with term t in it})
\end{equation}

Once we decide on the total number of top rare features, the rest of the steps remain same as before with the only change being that we compute the cosine similarity and difference in coefficients matrix for only these top features and add the resulting value as the word-vector regularization cost.

\newpage
\subsection{Model Performance}

Below is a table showing how our model performed as compared to the pyramid and tensorflow implementation of vanilla logistic regression.

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
Datasets    & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Pyramid LR\\ Set Accuracy\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}Tensorflow LR\\ Set Accuracy\end{tabular}} & \multicolumn{1}{l|}{\begin{tabular}[c]{@{}l@{}}LR + WV Reg\\ Set Accuracy\end{tabular}} \\ \hline
IMDb        & 19.66                                                                                  & 18.21                                                                                     & 17.19                                                                                   \\
20Newsgroup & 55.64                                                                                  & 55.75                                                                                     & 56.32                                                                                  
\end{tabular}
\caption{\label{tab:widgets}Set-Accuracy Results.}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{l|c|c|c|}
Datasets    & \begin{tabular}[c]{@{}c@{}}Pyramid LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}Tensorflow LR\\ Instance-F1\end{tabular} & \begin{tabular}[c]{@{}c@{}}LR + WV Reg\\ Instance-F1\end{tabular} \\ \hline
IMDb        & 56.14                                                            & 57.87                                                               & 47.67                                                             \\
20Newsgroup & 55.64                                                            & 55.75                                                               & 55.48                                                            
\end{tabular}
\caption{\label{tab:widgets}Instance-F1 Results.}
\end{table}

Based on the above model performance, we can see that the word-vector regularization model did not perform that well as compared to the vanilla logistic regression model. This could be due to the fact that there our model was restricted by the total number of rare features we could use to regularize the model along with the added regularization constraint.